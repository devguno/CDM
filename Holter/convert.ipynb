{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Report to XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from xml.etree.ElementTree import Element, SubElement, tostring\n",
    "from xml.dom.minidom import parseString\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_match(pattern, text, default=\"Unknown\"):\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group(1) if match else default\n",
    "\n",
    "def extract_grouped_matches(pattern, text, groups, default=\"Unknown\"):\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return [match.group(i + 1) for i in range(groups)]\n",
    "    return [default] * groups\n",
    "\n",
    "def parse_general_section(text):\n",
    "    general_section = re.search(r\"General\\n(.+?)Heart Rates\", text, re.DOTALL)\n",
    "    if general_section:\n",
    "        general_text = general_section.group(1)\n",
    "        qrs_complexes = extract_match(r\"(\\d+) QRS complexes\", general_text)\n",
    "        ventricular_beats = extract_match(r\"(\\d+) Ventricular beats\", general_text)\n",
    "        supraventricular_beats = extract_match(r\"(\\d+) Supraventricular beats\", general_text)\n",
    "        noise_percentage = extract_match(r\"(<\\s*\\d+|\\d+) % of total time classified as noise\", general_text, \"0\")\n",
    "        paced_beats = extract_match(r\"(\\d+) Paced beats\", general_text)\n",
    "        af_afl_percentage = extract_match(r\"(<\\s*\\d+|\\d+) % of total time in AF/AFL\", general_text)\n",
    "        bb_beats = extract_match(r\"(\\d+) BB beats\", general_text)\n",
    "        junctional_beats = extract_match(r\"(\\d+) Junctional beats\", general_text)\n",
    "        aberrant_beats = extract_match(r\"(\\d+) Aberrant beats\", general_text)\n",
    "    else:\n",
    "        qrs_complexes = ventricular_beats = supraventricular_beats = noise_percentage = \"Unknown\"\n",
    "        paced_beats = af_afl_percentage = bb_beats = junctional_beats = aberrant_beats = \"Unknown\"\n",
    "    return {\n",
    "        'QRScomplexes': qrs_complexes,\n",
    "        'VentricularBeats': ventricular_beats,\n",
    "        'SupraventricularBeats': supraventricular_beats,\n",
    "        'NoisePercentage': noise_percentage,\n",
    "        'PacedBeats': paced_beats,\n",
    "        'AFAFLPercentage': af_afl_percentage,\n",
    "        'BBBeats': bb_beats,\n",
    "        'JunctionalBeats': junctional_beats,\n",
    "        'AberrantBeats': aberrant_beats\n",
    "    }\n",
    "\n",
    "def parse_heart_rates_section(text):\n",
    "    heart_rates_data = {}\n",
    "    patterns = [\n",
    "        (r\"(\\d+) Minimum at ([\\d:]+ \\d+-\\w+)\", 'MinimumRate', 'Timestamp'),\n",
    "        (r\"(\\d+) Average\", 'AverageRate', None),\n",
    "        (r\"(\\d+) Maximum at ([\\d:]+ \\d+-\\w+)\", 'MaximumRate', 'Timestamp'),\n",
    "        (r\"(\\d+)\\s*Beats in tachycardia \\(>=?\\d+\\s*bpm\\),\\s*(\\d+)% total\", 'TachycardiaBeats', 'TachycardiaPercentage'),\n",
    "        (r\"(\\d+)\\s*Beats in bradycardia \\(<=?\\d+\\s*bpm\\),\\s*(\\d+)% total\", 'BradycardiaBeats', 'BradycardiaPercentage')\n",
    "    ]\n",
    "    for pattern, main_tag, sub_tag in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            heart_rates_data[main_tag] = (match.group(1), match.group(2) if sub_tag else None)\n",
    "        else:\n",
    "            heart_rates_data[main_tag] = (\"Unknown\", \"Unknown\" if sub_tag else None)\n",
    "    return heart_rates_data\n",
    "\n",
    "def parse_section(section_text, patterns):\n",
    "    section_data = {}\n",
    "    for pattern, tags in patterns:\n",
    "        matches = extract_grouped_matches(pattern, section_text, len(tags))\n",
    "        for tag_index, tag in enumerate(tags):\n",
    "            section_data[tag] = matches[tag_index]\n",
    "    return section_data\n",
    "\n",
    "def create_xml(patient_info, general_data, heart_rates_data, ventriculars_data, supraventriculars_data, xml_path):\n",
    "    root = Element('HolterReport')\n",
    "    patient_info_element = SubElement(root, 'PatientInfo')\n",
    "    for key, value in patient_info.items():\n",
    "        SubElement(patient_info_element, key).text = value\n",
    "\n",
    "    general_element = SubElement(root, 'General')\n",
    "    for key, value in general_data.items():\n",
    "        SubElement(general_element, key).text = value\n",
    "\n",
    "    heart_rates_element = SubElement(root, 'HeartRates')\n",
    "    for key, (value, sub_value) in heart_rates_data.items():\n",
    "        element = SubElement(heart_rates_element, key)\n",
    "        if sub_value:\n",
    "            SubElement(element, 'Timestamp').text = sub_value\n",
    "        element.text = value\n",
    "\n",
    "    ventriculars_element = SubElement(root, 'Ventriculars')\n",
    "    for key, value in ventriculars_data.items():\n",
    "        SubElement(ventriculars_element, key).text = value\n",
    "\n",
    "    supraventriculars_element = SubElement(root, 'Supraventriculars')\n",
    "    for key, value in supraventriculars_data.items():\n",
    "        SubElement(supraventriculars_element, key).text = value\n",
    "\n",
    "    xml_str = tostring(root, 'utf-8')\n",
    "    parsed_str = parseString(xml_str)\n",
    "    pretty_xml_str = parsed_str.toprettyxml(indent=\"   \")\n",
    "\n",
    "    with open(xml_path, \"w\") as xml_file:\n",
    "        xml_file.write(pretty_xml_str)\n",
    "\n",
    "def process_pdf_files(file_dirs, xml_dir):\n",
    "    pdf_files = []\n",
    "    for file_dir in file_dirs:\n",
    "        for root, _, files in os.walk(file_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.pdf'):\n",
    "                    pdf_files.append(os.path.join(root, file))\n",
    "    \n",
    "    failed_files = []\n",
    "\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing PDF Files\"):\n",
    "        try:\n",
    "            filename = os.path.basename(pdf_path)\n",
    "            pdf_doc = fitz.open(pdf_path)\n",
    "            page = pdf_doc.load_page(0)\n",
    "            extracted_text = page.get_text()\n",
    "\n",
    "            patient_info = {\n",
    "                'PID': extract_match(r\"Patient Name:?\\n(\\d+)\\nID:?\", extracted_text, filename.split('_')[-1].replace('.pdf', '')),\n",
    "                'HookupDate': extract_match(r\"Medications:?\\n(\\d+-\\w+-\\d+)\\nHookup Date:?\", extracted_text, \"Unknown\"),\n",
    "                'HookupTime': extract_match(r\"Hookup Date:?\\n(\\d+:\\d+:\\d+)\\nHookup Time:?\", extracted_text, \"Unknown\"),\n",
    "                'Duration': extract_match(r\"Hookup Time:?\\n(\\d+:\\d+:\\d+)\\nDuration:?\", extracted_text, \"Unknown\"),\n",
    "                'Age': extract_match(r\"(\\d+)\\s*yr\\s*Age:\", extracted_text, \"Unknown\"),\n",
    "                'Gender': extract_match(r\"(Male|Female)\\s*Gender:\", extracted_text, \"Unknown\")\n",
    "            }\n",
    "\n",
    "            general_data = parse_general_section(extracted_text)\n",
    "\n",
    "            heart_rates_data = parse_heart_rates_section(extracted_text)\n",
    "\n",
    "            ventriculars_section = extract_match(r\"Ventriculars \\(V, F, E, I\\)\\n([\\s\\S]+?)\\nSupraventriculars \\(S, J, A\\)\", extracted_text, \"\")\n",
    "            supraventriculars_section = extract_match(r\"Supraventriculars \\(S, J, A\\)\\n([\\s\\S]+?)Interpretation\", extracted_text, \"\")\n",
    "\n",
    "            ventriculars_patterns = [\n",
    "                (r\"(\\d+) Isolated\", ['Isolated']),\n",
    "                (r\"(\\d+) Couplets\", ['Couplets']),\n",
    "                (r\"(\\d+) Bigeminal cycles\", ['BigeminalCycles']),\n",
    "                (r\"(\\d+) Runs totaling (\\d+) beats\", ['Runs', 'TotalBeats']),\n",
    "                (r\"(\\d+) Beats longest run (\\d+) bpm ([\\d:]+ \\d+-\\w+)\", ['LongestRunBeats', 'LongestRunBPM', 'LongestRunTimestamp']),\n",
    "                (r\"(\\d+) Beats fastest run (\\d+) bpm ([\\d:]+ \\d+-\\w+)\", ['FastestRunBeats', 'FastestRunBPM', 'FastestRunTimestamp'])\n",
    "            ]\n",
    "\n",
    "            supraventriculars_patterns = [\n",
    "                (r\"(\\d+) Isolated\", ['Isolated']),\n",
    "                (r\"(\\d+) Couplets\", ['Couplets']),\n",
    "                (r\"(\\d+) Bigeminal cycles\", ['BigeminalCycles']),\n",
    "                (r\"(\\d+) Runs totaling (\\d+) beats\", ['Runs', 'TotalBeats']),\n",
    "                (r\"(\\d+) Beats longest run (\\d+) bpm ([\\d:]+ \\d+-\\w+)\", ['LongestRunBeats', 'LongestRunBPM', 'LongestRunTimestamp']),\n",
    "                (r\"(\\d+) Beats fastest run (\\d+) bpm ([\\d:]+ \\d+-\\w+)\", ['FastestRunBeats', 'FastestRunBPM', 'FastestRunTimestamp'])\n",
    "            ]\n",
    "\n",
    "            ventriculars_data = parse_section(ventriculars_section, ventriculars_patterns)\n",
    "            supraventriculars_data = parse_section(supraventriculars_section, supraventriculars_patterns)\n",
    "\n",
    "            xml_path = os.path.join(xml_dir, os.path.splitext(filename)[0] + '.xml')\n",
    "            create_xml(patient_info, general_data, heart_rates_data, ventriculars_data, supraventriculars_data, xml_path)\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {e}\")\n",
    "            failed_files.append(filename)\n",
    "\n",
    "    return failed_files\n",
    "\n",
    "def main():\n",
    "    base_dirs = [\n",
    "        r'C:\\old_rename'\n",
    "    ]\n",
    "    xml_dir = r'C:\\old_rename'\n",
    "\n",
    "    if not os.path.exists(xml_dir):\n",
    "        os.makedirs(xml_dir)\n",
    "\n",
    "    print(\"Starting to process PDF files...\")\n",
    "    failed_files_record = process_pdf_files(base_dirs, xml_dir)\n",
    "\n",
    "    if failed_files_record:\n",
    "        print(\"\\nFailed to process the following files:\")\n",
    "        for failed_file in failed_files_record:\n",
    "            print(failed_file)\n",
    "    else:\n",
    "        print(\"\\nAll PDF files processed successfully.\")\n",
    "\n",
    "    print(\"Completed processing all files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  XML 파일을 읽고, PID 값을 파일명에 있는 값으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 폴더 경로\n",
    "folder_path = r'C:\\old_rename'\n",
    "\n",
    "# 폴더 내의 모든 XML 파일을 확인\n",
    "xml_files = [file for file in os.listdir(folder_path) if file.endswith('.xml')]\n",
    "\n",
    "# tqdm을 사용하여 진행 상황 표시\n",
    "for file_name in tqdm(xml_files, desc=\"Processing XML files\"):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # 파일명에서 '_' 뒤의 값을 추출\n",
    "    new_pid = file_name.split('_')[-1].split('.')[0]\n",
    "    \n",
    "    # XML 파일 로드\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # PID 태그를 찾고 값을 새로운 PID로 변경\n",
    "    for pid in root.iter('PID'):\n",
    "        pid.text = new_pid\n",
    "    \n",
    "    # 변경된 XML을 파일에 다시 저장\n",
    "    tree.write(file_path)\n",
    "\n",
    "print(\"모든 XML 파일의 PID 값이 변경되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PID to CDM_ID File Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil  # 파일 이동을 위해 shutil 모듈 사용\n",
    "\n",
    "# 경로 설정\n",
    "csv_path = r'C:\\github\\CDM\\Holter\\holter_pid.csv'\n",
    "file_dir = r'C:\\old_sig'\n",
    "rename_dir = r'c:\\old_rename'\n",
    "\n",
    "# old_rename 폴더가 없으면 생성\n",
    "if not os.path.exists(rename_dir):\n",
    "    os.makedirs(rename_dir)\n",
    "\n",
    "# CSV 파일 로드 및 딕셔너리 생성\n",
    "df = pd.read_csv(csv_path)\n",
    "pid_to_cdm_id = df.set_index('pid')['cdm_id'].to_dict()\n",
    "\n",
    "# 파일 리스트 가져오기\n",
    "files = [f for f in os.listdir(file_dir) if '_' in f]\n",
    "\n",
    "converted_count = 0\n",
    "\n",
    "# 파일 이름 변경 및 이동\n",
    "for file in tqdm(files, desc=\"Renaming files\"):\n",
    "    try:\n",
    "        parts = file.split('_')\n",
    "        index_number, pid = parts[0], parts[1].split('.')[0]  # 파일명에서 인덱스 번호와 pid 추출\n",
    "        extension = parts[1].split('.')[1]  # 파일 확장자 추출\n",
    "        cdm_id = pid_to_cdm_id.get(int(pid))  # pid에 대응하는 cdm_id 찾기\n",
    "\n",
    "        if cdm_id:\n",
    "            # cdm_id를 사용해 새로운 파일명 생성\n",
    "            new_filename = f'{index_number}_{int(cdm_id)}.{extension}'  \n",
    "\n",
    "            old_file_path = os.path.join(file_dir, file)\n",
    "            new_file_path = os.path.join(rename_dir, new_filename)\n",
    "            # 파일 이름 변경 후 이동\n",
    "            shutil.move(old_file_path, new_file_path)\n",
    "            converted_count += 1  \n",
    "    except ValueError:\n",
    "        print(f'Error processing file: {file}')\n",
    "\n",
    "print(f'Task completed. A total of {converted_count} files were converted and moved to {rename_dir}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update .hea Filename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def replace_filename_in_hea_files(directory):\n",
    "    converted_files_count = 0\n",
    "    \n",
    "    # Traverse the directory and find .hea files\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('.hea'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    lines = file.readlines()\n",
    "                \n",
    "                # Modify the line\n",
    "                base_filename = filename.replace('.hea', '')\n",
    "                lines[0] = lines[0].replace(lines[0].split()[0], base_filename)\n",
    "                \n",
    "                for i in range(1, len(lines)):\n",
    "                    lines[i] = lines[i].replace(lines[i].split()[0], f\"{base_filename}.SIG\")\n",
    "                \n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.writelines(lines)\n",
    "                \n",
    "                converted_files_count += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {filename}. Error: {e}\")\n",
    "    \n",
    "    print(f\"Total number of converted files: {converted_files_count}\")\n",
    "\n",
    "directory_path = r'C:\\old_rename'\n",
    "replace_filename_in_hea_files(directory_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
